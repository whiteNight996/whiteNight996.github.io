<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark提交参数说明和常见优化]]></title>
    <url>%2F2019%2F11%2F26%2FSpark%E6%8F%90%E4%BA%A4%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E%E5%92%8C%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[MR之所以慢是因为每一次操作数据都写在了磁盘上，大量的IO造成了时间和资源的浪费，但是Spark是基于内存的计算引擎，相比MR，减少的是大量的IO； 但并不是说给一个Spark程序足够的资源，就可以为所欲为了，在提交一个spark程序时，不仅要考虑所在资源队列的总体情况，还要考虑代码本身的高效性，要尽量避免大量的shuffle操作和action操作，尽量使用同一个rdd。 1bin/spark-submit --help Spark提交任务时常见的两种模式 1）local/local[K] 本地使用1/K个 worker 线程运行 spark 程序； 2）yarn-client/yarn-cluster 以client/cluster方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver在client/cluster运行。 在提交任务时的几个重要参数 executor-cores —— 每个executor使用的内核数，默认为1，官方建议2-5个；num-executors —— 启动executors的数量，默认为2；executor-memory —— executor内存大小，默认1G；driver-cores —— driver使用内核数，默认为1；driver-memory —— driver内存大小，默认512M。 12345678910111213spark-submit \ --master local[5] \ --driver-cores 2 \ --driver-memory 8g \ --executor-cores 4 \ --num-executors 10 \ --executor-memory 8g \ --class PackageName.ClassName XXXX.jar \ --name "Spark Job Name" \ InputPath \ OutputPath如果这里通过--queue 指定了队列，那么可以免去写--master 常规注意事项 预处理数据，丢掉一些不必要的数据； 增加Task的数量； 过滤掉一些容易导致发生倾斜的key； 避免创建重复的RDD，尽可能复用一个RDD，对多次使用的RDD进行持久化； 在要使用groupByKey算子的时候,尽量用reduceByKey或者aggregateByKey算子替代.因为调用groupByKey时候,按照相同的key进行分组,形成RDD[key,Iterable[value]]的形式,此时所有的键值对都将被重新洗牌,移动,对网络数据传输造成理论上的最大影响； 使用高性能的算子。]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据仓库建模]]></title>
    <url>%2F2019%2F11%2F25%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%2F</url>
    <content type="text"><![CDATA[数仓建模的意义数据模型就是数据组织和存储方法，它强调从业务、数据存取和使用角度合理存储数据。 Linux的创始人Torvalds有一段关于“什么才是优秀程序员”的话：“烂程序员关心的是代码，好程序员关心的是数据结构和它们之间的关系”，最能够说明数据模型的重要性。 只有数据模型将数据有序的组织和存储起来以后，大数据才能得到高性能、低成本、高效率、高质量的使用。 性能：帮助我们快速查询所需要的数据，减少数据的I/O吞吐量，提高使用数据的效率，如宽表。 成本：极大的减少不必要的数据冗余，也能实现计算结果复用，极大地降低存储和计算成本； 效率：在业务系统发生变化的时候，可以保持稳定或很同意扩展，提高数据稳定性和连续性； 质量：良好的数据模型能改善数据统计口径的不一致性，减少数据计算错误的可能性。 数据模型可以促进业务与技术进行有效沟通，形成对主要业务定义和术语的统一认识。 大数据系统需要数据模型方法来更好的组织和存储数据。 比如：通过统一数据模型，屏蔽数据源变化对业务的影响，保证业务的稳定。 数据仓库分层的设计 清晰数据结构：每一个数据分层都有它的作用于，这样在使用表的时候可以更方便的定位和理解； 数据血缘追踪：能够快速准确的定位问题，并清楚他的危害范围； 减少重复开发：规范数据分层，开发一些通用的中间层数据，能减少极大的重复计算； 把复杂问题简单化：将复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。当数据出现问题之后，不需要修复所有的数据，只需要从有问题的步骤开始修复； 屏蔽原始数据的异常：不必改一次业务就需要重新接入数据。 一般的，分为4层： ODS层：数据结构与源数据系统完全一致； DWD层：企业数据资产的汇聚层。数据经过标准化处理后按照数据仓库的主题域组织存放，保留细节和历史数据； DWS层：各业务单元使用的关键数据进行整合，整合了一定的业务知识。建模方式采用星型或雪花模型设计的多维数据结构（维度 模型）、数据宽表、维度退化 | 关系型数据结构业务宽表。 ADS层：直接面向各应用，建模方式采用星型或雪花模型设计的多维数据结构（维度模型）、数据宽表、维度退化。 数据粒度：详细粒度数据 – 详细粒度数据 – 多维度汇总数据/实例级汇总数据 – 高度汇总数据 两种经典的数据仓库建模方法关系建模和维度建模； 维度建模1）定义维度建模以分析决策的需求出发构建模型，构建的数据模型为分析需求服务，因此它重点解决用户如何更快速完成分析需求，同时还有较好的大规模复杂查询的响应性能，更直接面向业务。典型的代表是我们比较熟知的星形模型。 星型模型由一个事实表和一组维表组成。每个维表都有一个维作为主键，所有这些维的主键组合成事实表的主键。强调的是对维度进行预处理，将多个维度集合到一个事实表，形成一个宽表； 这也是我们在使用Hive时，经常会看到一些大宽表的原因，大宽表一般都是事实表，包含了维度关联的主键和一些度量信息，而维度表则是事实表里面维度的具体信息，使用时候一般通过join来组合数据，相对来说对OLAP的分析比较方便。 2）建模方法 通常需要选择某个业务过程，然后围绕该过程建立模型，其一般采用自底向上的方法，从明确关键业务过程开始，再到明确粒度，再到明确维度，最后明确事实，非常简单易懂。 选择业务过程：可以是某个业务时间，也可以是某个事件的状态，还可以是多个事件组成的流程； 明确粒度：在事件分析中，预判所有分析的细分程度，然后决定选择的粒度； 识别维表：基于选择的粒度设计维表，分析维表的具体属性； 选择事实：确定需要衡量的指标。 以下是阿里的OneData的建模工作流，可以参考。 3）优缺点 优点：更快速完成分析需求，较好的大规模复杂查询的响应性能；缺点：维度表的冗余会较多 关系建模1）定义是数据仓库之父Inmon推崇的、从全企业的高度设计一个3NF模型的方法，用实体加关系描述的数据模型描述企业业务架构，在范式理论上符合3NF，站在企业角度面向主题的抽象，而不是针对某个具体业务流程的实体对象关系抽象。 它更多的是面向数据的整合和一致性治理，正如Inmon所希望达到的“single version of the truth”。 当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。 雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 “层次 “ 区域，这些被分解的表都连接到主维度表而不是事实表。 雪花模型更加符合数据库范式，减少数据冗余，但是在分析数据的时候，操作比较复杂，需要join的表比较多所以其性能并不一定比星型模型高。 2）建模方法 关系建模常常需要全局考虑，要对上游业务系统的进行信息调研，以做到对其业务和数据的基本了解，要做到主题划分，让模型有清晰合理的实体关系体系 前期准备 – 源系统分析 – 模型概要设计 – 模型详细设计 – 模型验证（文档、跟踪、交流、验证、测试、确认） 3）优缺点优点：规范性较好，冗余小，数据集成和数据一致性方面得到重视，比如运营商可以参考国际电信运营业务流程规范（ETOM），有所谓的最佳实践。 缺点：需要全面了解企业业务、数据和关系；实施周期非常长，成本昂贵；对建模人员的能力要求也非常高，容易烂尾。 建模方法比较一般来讲，维度模型简单直观，适合业务模式快速变化的行业，关系模型实现复杂，适合业务模式比较成熟的行业，阿里原来用关系建模，现在基本都是维度建模的方式了。 3.4 企业建模的三点经验 维度建模就不说了，只要能理解业务过程和其中涉及的相关数据、维度就可以，但自顶向下的关系建模难度很大，以下是关系建模的三个建设要点。 业务的理解：找到企业内最理解业务和源系统的人，梳理出现状； 数据及关系的理解：各个域的系统建设的时候没有统一文档和规范，要梳理出逻辑模型不容易 标准化的推进：数据仓库建模的任何实体都需要标准化命名，否则未来的管理成本巨大，也是后续数据有效治理的基础]]></content>
  </entry>
  <entry>
    <title><![CDATA[浅析Scala之隐式转换]]></title>
    <url>%2F2019%2F09%2F26%2F%E6%B5%85%E6%9E%90Scala%E4%B9%8B%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[起因先说一下OCP开发原则，(Open Close Principle) 开闭原则的含义是说一个软件实体应该通过扩展来实现变化，而不是通过修改已有代码来实现变化。Scala中的隐式转换与之貌离神合：在不修改代码的情况下，让编译通过，和扩展功能类似。 举例说明，开发中我们经常会做版本的迭代更新，如果之前xxx0.1版本兼容性很好，在迭代xxx0.2版本中因为考虑欠妥导致某些功能不兼容，这时候难道要重写整个xxx0.2版本吗？这个问题本身要解决的，就是想办法让xxx0.2版本jar包不变情况下，程序可以正常工作。事实上，我们要做的就是将相关依赖的jar包更新。 是什么Scala编译器在编译代码时发生错误（第一次），然后编译器会在指定的范围内查找对应的转换规则，看是否能再次通过，Scala将这种操作称为隐式转换 应用：Java中借鉴了这种思想。比如在Java中对象之间转换，需要有父子（继承）关系；但是基本数据类型之间也可以进行转换，其实背后是由编译器完成的。 Tips：编译器帮你做的事，不违背OCP开发原则。隐式解析机制（1）首先会在当前代码作用域下查找隐式实体（隐式方法、隐式类、隐式对象）。（2）如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找。类型的作用域是指与该类型相关联的全部伴生对象以及该类型所在包的包对象。 Tips：同一作用域如果出现两个或以上的功能类似的隐式转换，就会报错，即隐式转换中不存在动态绑定。Scala中具体实现普通方法或者函数可以通过implicit关键字声明隐式参数，调用该方法时，就可以传入该参数，编译器会再相应的作用域寻找符合条件的隐式值。 （1）隐式函数 12345678910def main(args: Array[String]): Unit = &#123; //定义一个隐式函数 implicit def tansform(d:Double):Int =&#123; d.toInt &#125; var i :Int = 5.0 //正常情况下会报错,在添加完隐式函数就编译通过了 println(i)&#125; (2)隐式参数 同一个作用域中，相同类型的隐式值只能有一个 编译器按照隐式参数的类型去寻找对应类型的隐式值，与隐式值的名称无关。 隐式参数优先于默认参数 12345678def regUser(implicit pwd:String = "123456"):Unit = &#123; //默认情况下，登录用户密码是123456,设置隐式参数 println("PWD:" + pwd) &#125; //隐式变量 implicit val password = "admin" regUser //使用隐式参数或变量时，调用要省略（） //输出结果：PWD:admin (3)隐式类在Scala2.10后提供了隐式类，可以使用implicit声明类，隐式类的非常强大，同样可以扩展类的功能，在集合中隐式类会发挥重要的作用。 其所带的构造参数有且只能有一个 隐式类必须被定义在“类”或“伴生对象”或“包对象”里，即隐式类不能是顶级的 12345678910111213141516def main(args: Array[String]): Unit = &#123; var a = new A a.insertData() a.updateData() &#125; class A&#123; def insertData()=&#123; println("insert data ...") &#125; &#125; implicit class B(a:A)&#123; //隐式类 def updateData()=&#123; println("update data ...") &#125; &#125; 1Hope is good thing, maybe the best of things. And no good thing ever dies。 --&lt;肖申克的救赎&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[JDK1.8源码分析-HashMap]]></title>
    <url>%2F2019%2F09%2F23%2FJDK1-8%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-HashMap%2F</url>
    <content type="text"><![CDATA[Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： HashMap简单介绍它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。 HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 要搞清楚HashMap， 首先要知道HashMap是什么，即它的存储结构-字段； 其次弄明白它能干什么，即它的功能实现-方法。 存储结构-字段从结构上来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的。 这里需要明白两个问题： 数据底层具体存储的是什么？ 这样的存储方式有什么优点？ （1）从源码可知，HashMap中有一个非常重要的字段，就是Node&lt;K,V&gt;[] table，即哈希桶数组。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; ... &#125; public final K getKey()&#123; ... &#125; public final V getValue() &#123; ... &#125; public final String toString() &#123; ... &#125; public final int hashCode() &#123; ... &#125; public final V setValue(V newValue) &#123; ... &#125; public final boolean equals(Object o) &#123; ... &#125;&#125; Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 （2）HashMap就是使用哈希表来存储的。哈希表为解决冲突，在Java中HashMap采用了链地址法。 简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put("张三","数据分析狮"); 系统将调用”张三”这个key的hashCode()方法得到其hashCode 值，然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置。 有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？ ​ 答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： 1234int threshold; // 阈值，所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。 其中，threshold = length * Load factor 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。 modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数，相对来说素数导致冲突的概率要小于合数。 HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。 功能实现-方法HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 1. 确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。HashMap定位数组索引位置，直接决定了hash方法的离散性能。源码如下（方法1+ 方法2）： 123456789101112方法一： static final int hash(Object key) &#123;//jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125;方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。 但是，模运算的消耗还是比较大的，**在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 1234567//保证下面小格子全能取到，不会浪费,所以容量必须是2^n; 目的：散列，均衡private static int roundUpToPowerOf2(int number) &#123; // assert number &gt;= 0 : "number must be non-negative"; return number &gt;= MAXIMUM_CAPACITY ? MAXIMUM_CAPACITY : (number &gt; 1) ? Integer.highestOneBit((number - 1) &lt;&lt; 1) : 1;&#125; 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的： (h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，这么做可以在数组table的length比较小的时候，也能保证高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 2. 分析HashMap的put方法HashMap的put方法执行过程可以通过下图来理解： JDK1.8HashMap的put方法源码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 1 public V put(K key, V value) &#123; 2 // 对key的hashCode()做hash 3 return putVal(hash(key), key, value, false, true); 4 &#125; 5 6 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, 7 boolean evict) &#123; 8 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; 9 // 步骤①：tab为空则创建10 if ((tab = table) == null || (n = tab.length) == 0)11 n = (tab = resize()).length;12 // 步骤②：计算index，并对null做处理 13 if ((p = tab[i = (n - 1) &amp; hash]) == null) 14 tab[i] = newNode(hash, key, value, null); //直接插入15 else &#123;16 Node&lt;K,V&gt; e; K k;17 // 步骤③：节点key存在，直接覆盖value18 if (p.hash == hash &amp;&amp;19 ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))20 e = p;21 // 步骤④：判断该链为红黑树22 else if (p instanceof TreeNode)23 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);24 // 步骤⑤：该链为链表25 else &#123;26 for (int binCount = 0; ; ++binCount) &#123;27 if ((e = p.next) == null) &#123;28 p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理29 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st 30 treeifyBin(tab, hash);31 break;32 &#125; // key已经存在直接覆盖value33 if (e.hash == hash &amp;&amp;34 ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) 35 break;36 p = e;37 &#125;38 &#125;39 40 if (e != null) &#123; // existing mapping for key41 V oldValue = e.value;42 if (!onlyIfAbsent || oldValue == null)43 e.value = value;44 afterNodeAccess(e);45 return oldValue;46 &#125;47 &#125;48 ++modCount;49 // 步骤⑥：超过最大容量 就扩容50 if (++size &gt; threshold)51 resize();52 afterNodeInsertion(evict);53 return null;54 &#125; 3. 扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 123456789101112131 void resize(int newCapacity) &#123; //传入新的容量 2 Entry[] oldTable = table; //引用扩容前的Entry数组 3 int oldCapacity = oldTable.length; 4 if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 5 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容 6 return; 7 &#125; 8 9 Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组10 transfer(newTable); //！！将数据转移到新的Entry数组里11 table = newTable; //HashMap的table属性引用新的Entry数组12 threshold = (int)(newCapacity * loadFactor);//修改阈值13 &#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 123456789101112131415161718 1 void transfer(Entry[] newTable) &#123; 2 Entry[] src = table; //src引用了旧的Entry数组 3 int newCapacity = newTable.length; 4 for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 5 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 6 if (e != null) &#123; 7 src[j] = null; //释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） 8 do &#123; 9 Entry&lt;K,V&gt; next = e.next;10 int i = indexFor(e.hash, newCapacity); //！重新计算每个元素在数组中的位置11 e.next = newTable[i]; //标记[1]12 newTable[i] = e; //将元素放在数组上13 e = next; //访问下一个Entry链上的元素14 &#125; while (e != null);15 &#125;16 &#125;17 &#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以当key = 3、7、5时，put顺序依次为 5、7、3，在mod 2以后都冲突在table[1]这里了。 这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。如下图所示： 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”,可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。 下面是JDK1.8resize扩容的源码，有兴趣的同学可以看一看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 1 final Node&lt;K,V&gt;[] resize() &#123; 2 Node&lt;K,V&gt;[] oldTab = table; 3 int oldCap = (oldTab == null) ? 0 : oldTab.length; 4 int oldThr = threshold; 5 int newCap, newThr = 0; 6 if (oldCap &gt; 0) &#123; 7 // 超过最大值就不再扩充了，就只好随你碰撞去吧 8 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; 9 threshold = Integer.MAX_VALUE;10 return oldTab;11 &#125;12 // 没超过最大值，就扩充为原来的2倍13 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp;14 oldCap &gt;= DEFAULT_INITIAL_CAPACITY)15 newThr = oldThr &lt;&lt; 1; // double threshold16 &#125;17 else if (oldThr &gt; 0) // initial capacity was placed in threshold18 newCap = oldThr;19 else &#123; // zero initial threshold signifies using defaults20 newCap = DEFAULT_INITIAL_CAPACITY;21 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);22 &#125;23 // 计算新的resize上限24 if (newThr == 0) &#123;25 26 float ft = (float)newCap * loadFactor;27 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ?28 (int)ft : Integer.MAX_VALUE);29 &#125;30 threshold = newThr;31 @SuppressWarnings(&#123;"rawtypes"，"unchecked"&#125;)32 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];33 table = newTab;34 if (oldTab != null) &#123;35 // 把每个bucket都移动到新的buckets中36 for (int j = 0; j &lt; oldCap; ++j) &#123;37 Node&lt;K,V&gt; e;38 if ((e = oldTab[j]) != null) &#123;39 oldTab[j] = null;40 if (e.next == null)41 newTab[e.hash &amp; (newCap - 1)] = e;42 else if (e instanceof TreeNode)43 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap);44 else &#123; // 链表优化重hash的代码块45 Node&lt;K,V&gt; loHead = null, loTail = null;46 Node&lt;K,V&gt; hiHead = null, hiTail = null;47 Node&lt;K,V&gt; next;48 do &#123;49 next = e.next;50 // 原索引51 if ((e.hash &amp; oldCap) == 0) &#123;52 if (loTail == null)53 loHead = e;54 else55 loTail.next = e;56 loTail = e;57 &#125;58 // 原索引+oldCap59 else &#123;60 if (hiTail == null)61 hiHead = e;62 else63 hiTail.next = e;64 hiTail = e;65 &#125;66 &#125; while ((e = next) != null);67 // 原索引放到bucket里68 if (loTail != null) &#123;69 loTail.next = null;70 newTab[j] = loHead;71 &#125;72 // 原索引+oldCap放到bucket里73 if (hiTail != null) &#123;74 hiTail.next = null;75 newTab[j + oldCap] = hiHead;76 &#125;77 &#125;78 &#125;79 &#125;80 &#125;81 return newTab;82 &#125; 1本文来源：美团技术 https://tech.meituan.com/2016/06/24/java-hashmap.html]]></content>
      <categories>
        <category>Java内功心法</category>
      </categories>
      <tags>
        <tag>JDK源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-JVM是如何加载类的?]]></title>
    <url>%2F2019%2F09%2F23%2FJava-JVM%E6%98%AF%E5%A6%82%E4%BD%95%E5%8A%A0%E8%BD%BD%E7%B1%BB%E7%9A%84%2F</url>
    <content type="text"><![CDATA[JVM是如何加载Java类的？先来讲讲盖房子这件事，首先你要请个建筑师给个方案，然后到市政部门报备、验证，通过后才可以开始盖房子，盖好房子还要装修，之后才能住人。 盖房子这个事，其实和Java虚拟机中的类加载很像。 从class文件到内存中的类，按先后顺序需要经过加载、链接以及初始化三大步骤。其中，链接过程也需要验证；而且内存中的类没有经过初始化，同样不能使用。 Java虚拟机中有两类数据类型：基本类型和引用类型。其中，Java的基本类型是由Java虚拟机预先定义好的。 至于引用类型，细分为四种：类、接口、数组类和泛型参数。其中泛型参数会在编译过程中被擦除（后面会更新详细过程）；数组类是由Java虚拟机直接生成的，其它两种则有对应的字节流。 说到字节流，我们可以在程序内直接生成，也可以从网络中获取。这些字节流都会被加载到Java虚拟机中。 Tips：无论是直接生成的数组类，还是加载的类，都需要JVM对其进行链接和初始化。 加载加载，是指查找字节流，并据此创建类的过程。对于数组类来说，由JVM直接生成；对于其它的类来说，JVM则需要借助类加载器来完成查找字节流的过程。 以盖房子为例，盖房子，你需要让建筑师设计一个房型，这个房型就相当于类，而建筑师就是类加载器。 启动类加载器(boot class loader)：是所有类加载器的父类。 扩展类加载器(extension class loader) 应用类加载器(application class loader) 规则：建筑师有个“潜规则”，就是接到单子不能着手干，而是先给师傅过过目，师傅不接手的情况下才能自己来。 这个规则叫做“双亲委派模型”。 每当一个类加载器接收到加载请求的时候，它会先将请求转发给父类加载器，在父类加载器没有找到所请求的类的情况下，该类加载器才会尝试去加载。 除了加载功能之外，加载器还提供了 命名空间的作用。这个比较好理解，开源产品也有版权的思维，你剽窃了我的想法，但是只要标记了自己的名字，这两个就是不同的。 在Java虚拟机汇总，类的唯一性是由类加载器实例以及类的全名一同确定的。即便是同一串字节流，经过不同的类加载器，也会得到两个不同的类。 链接链接，就是将创建成的类合并至Java虚拟机中，使之能够执行的过程。它分为：验证、准备和解析三个阶段。 验证的目的，在于确保加载类能够满足Java虚拟机的约束条件。这就好比将设计好的房型交给市政部门审核，只有审核通过，才能继续下面的建造工作。 准备阶段的目的，则是为被加载类的静态字段分配内存。Java代码中对静态字段的具体初始化，则在稍后的初始化阶段中进行。过了正极端。算是改好了毛坯房，虽然结构完整，但是没有装修之前是不能住人的。 除了分配内存之外，还会在此阶段构造跟类层次相关的数据结构，比如用来实现虚方法的动态绑定的方法表。 在class文件被加载至JVM中之前，这个类无法知道其它类的地址，甚至不知道自己的方法、字段的地址。因此，当要引用这些成员时，Java编译器会为其生成一个符号引用。在运行阶段，这个符号引用一般能够无歧义的定位到具体目标上。 解析阶段的目的，正式将这些符号引用解析成为实际引用。如果符号引用指向一个未被加载的类，或者未被加载类的字段或方法，那么解析将会触发这个类的加载（但未必会触发这个类的链接和初始化）。 Tips：Java虚拟机规范没有要求在链接过程中完成解析，只是规定了，如果某些字节码使用了符号引用，在执行这些字节码之前，需要完成对这些符号引用的解析。 初始化对于静态字段，可以显式赋值，或者在静态代码块中赋值。 如果显式赋值的字段被final修饰，并且其类型是基本类型或字符串时，那么该字段会被Java编译器标记为常量值（ConstantValue）,其初始化直接由Java虚拟机完成。除此之外的直接赋值以及静态代码块中代码，都会在方法中执行。 Tips：Java虚拟机会通过加锁来确保类的方法仅被执行一次。 只有当初始化完成之后，类才正式成为可执行的状态。也就是，只有房子装修完后，我们才能住进去。 那么，类的初始化何时会被触发呢？ 当虚拟机启动时，初始化用户指定的主类； 当new对象的指令时候； 当遇到调用静态方法的指令时，初始化该静态方法所在的类； 当遇到调用静态字段的指令时，初始化该静态字段所在的类； 子类的初始化会触发父类的初始化； default方法会触发该接口的初始化； 使用反射API对某个类进行反射调用时，初始化这个类。 1本文来源：郑雨迪-- 深入拆解Java虚拟机]]></content>
      <categories>
        <category>Java内功心法</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-类初始化和实例初始化过程]]></title>
    <url>%2F2019%2F09%2F22%2FJava-%E7%B1%BB%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[实例初始化在介绍实例初始化之前，先复习一下非静态代码块的知识。首先，非静态代码块在什么时候执行的？ ①非静态代码块是在创建对象的时候执行的； ②非静态代码块的执行顺序在构造器之前。 OK，了解了这个概念之后，我们说一下实例初始化过程，它就是创建对象时，为对象进行初始化的操作。 其中，初始化一般包括三部分： ①为成员变量显式赋值； ② 执行非静态代码块； ③执行构造器； Java编译器会将这三部分，合成一个() 或(形参列表)的方法，这个方法叫做 实例初始化方法 。 Tips：其实，编译后的class字节码信息中，是没有构造器这个概念的。 注意： 有几个构造器，就会有几个实例初始化方法。那么当你创建对象，调用对应的构造器时，实际执行的是对应的实例初始化方法。 上述的①和②按照顺序执行，③最后执行 实例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class TestBlock &#123; public static void main(String[] args) &#123;/* MyClass my1 = new MyClass();//调用无参构造 MyClass my2 = new MyClass("atguigu");//调用有参构造*/ Demo d1 = new Demo();//调用无参构造，本质上是调用&lt;init&gt;()实例初始化方法 Demo d2 = new Demo("atguigu");//调用有参构造，本质上是调用&lt;init&gt;(形参列表)实例初始化方法 &#125;&#125;class MyClass&#123; private String str = "hello";//显式赋值 public MyClass()&#123; System.out.println("无参构造"); &#125; public MyClass(String str)&#123; this.str = str; System.out.println("有参构造"); &#125; &#123; System.out.println("非静态代码块"); &#125;&#125;class Demo&#123; &#123; System.out.println("非静态代码块1"); &#125; private String str = assign();//调用方法，来为str进行显式赋值 public Demo()&#123; System.out.println("无参构造"); &#125; public Demo(String str)&#123; this.str = str; System.out.println("有参构造"); &#125; &#123; System.out.println("非静态代码块2"); &#125; public String assign()&#123; System.out.println("assign方法"); return "hello"; &#125;&#125;//Demo输出结果 //非静态代码块1 //assign方法 //非静态代码块2 //无参构造 或 有参构造 那么继承关系中，实例初始化过程是怎么执行的呢？ 子类的构造器一定会调用父类的构造器，默认是父类的无参构造，隐含了super()方法，本质上是执行了父类的实例初始化方法。 先执行父类的实例初始化方法； 再执行子类的实例初始化方法 Tips：this关键字在构造器，或者说在实力初始化方法中，表示的是正在创建的对象。 示例代码： 123456789101112131415161718192021222324252627282930313233public class TestInit2 &#123; public static void main(String[] args) &#123; Zi z = new Zi();//312645 &#125;&#125;class Fu&#123; private String strFu = assignFu(); &#123; System.out.println("(1)父类的非静态代码块"); &#125; public Fu()&#123; System.out.println("(2)父类的无参构造"); &#125; public String assignFu()&#123; System.out.println("(3)父类的assignFu()"); return "fu"; &#125;&#125;class Zi extends Fu&#123; private String strZi = assignZi(); &#123; System.out.println("(4)子类的非静态代码块"); &#125; public Zi()&#123; //super() ==&gt;调用父类的实例初始化方法，而且它在子类实例初始化方法的首行 System.out.println("(5)子类的无参构造"); &#125; public String assignZi()&#123; System.out.println("(6)子类的assignZi()"); return "zi"; &#125;&#125; 如果考虑子类中重写了父类中的方法，那么在父类的实例初始化方法中会调用重写的方法。 1234567891011121314151617181920212223242526272829303132333435public class TestInit3 &#123; public static void main(String[] args) &#123; Er r = new Er();//612645，因为子类重写了assign() // Ba b = new Ba(); &#125;&#125;class Ba&#123; private String str = assign(); &#123; System.out.println("(1)父类的非静态代码块"); &#125; public Ba()&#123; System.out.println("(2)父类的无参构造"); &#125; public String assign()&#123; System.out.println("(3)父类的assign()"); return "ba"; &#125;&#125;class Er extends Ba&#123; private String str = assign(); &#123; System.out.println("(4)子类的非静态代码块"); &#125; public Er()&#123; //super() ==&gt;调用父类的实例初始化方法，而且它在子类实例初始化方法的首行 System.out.println("(5)子类的无参构造"); &#125; public String assign()&#123; System.out.println("(6)子类的assign()"); return "er"; &#125;&#125; 输出结果 12345678/*(6)子类的assign()(1)父类的非静态代码块(2)父类的无参构造(6)子类的assign()(4)子类的非静态代码块(5)子类的无参构造*/ 类初始化和上面思路一样，我们先说一下静态代码块的特点： ①每个类的静态代码块只会执行一次； ②静态代码块在第一次使用这个类之前调用，即在类初始化时执行。 Tips：类的加载不一定会执行初始化 类初始化方法：()或者(形参列表) 一个类可能有好几个实例初始化方法，但是类初始化方法只有一个，由两部分组成： ①静态变量的显式赋值； ②静态代码块的代码。 其中①②按照顺序执行，谁在前面谁先执行。 示例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestStatic &#123; static&#123; System.out.println("TestStatic静态代码块"); &#125; public static void main(String[] args) &#123; /*MyClass.test(); MyClass.test(); MyClass.test(); MyClass.test();*/ /* MyClass my1 = new MyClass(); MyClass my2 = new MyClass(); MyClass my3 = new MyClass();*/ Demo d = new Demo();//现在第一次使用Demo类，用它创建一个对象时，会导致Demo类的初始化 &#125;&#125;class MyClass&#123; static&#123; System.out.println("MyClass静态代码块"); &#125; public static void test()&#123; System.out.println("静态方法"); &#125; &#125;class Demo&#123; static&#123; System.out.println("（1）Demo的静态代码块1"); &#125; private static String info = assign(); static&#123; System.out.println(info); System.out.println("（2）Demo的静态代码块2"); &#125; public static String assign()&#123; System.out.println(info); System.out.println("（3）assign()方法"); return "hello"; &#125;&#125; 考虑继承的关系，类初始化又是怎么执行的呢？ （1）先执行父类的类初始化 （2）执行子类的类初始化 Tips：静态方法是不会被重写的。因为静态方法在编译期就确定了，不需要在运行时动态绑定。 123456789101112131415161718192021222324252627282930313233public class TestClassInit &#123; public static void main(String[] args) &#123; /*Father f = new Father(); System.out.println("-------------------"); Son s = new Son();*/ Son s = new Son(); /*Father f = new Son();//多态引用 System.out.println("----------------"); f.assign();//静态方法在编译期间就确定了，不需要在运行时动态绑定*/ &#125;&#125;class Father&#123; private static String info = assign(); static&#123; System.out.println("(1)父类的静态代码块"); &#125; public static String assign()&#123; System.out.println("（3）assign()方法"); return "Father"; &#125;&#125;class Son extends Father&#123; private static String info = assign(); static&#123; System.out.println("(2)子类的静态代码块"); &#125; public static String assign()&#123; System.out.println("（4）assign()方法"); return "Son"; &#125;&#125; 总结在创建对象的时候，会先执行类初始化，包括：①类变量的显式赋值②静态代码块； 再去执行实例化方法（①非静态变量的显式赋值②非静态代码块③构造器【1.无参–2.有参】）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class TestExer &#123; public static void main(String[] args) &#123; Zi zi = new Zi(); &#125;&#125;class Fu&#123; private static int i = getNum("（1）i"); private int j = getNum("（2）j"); static&#123; print("（3）父类静态代码块"); &#125; &#123; print("（4）父类非静态代码块，又称为构造代码块"); &#125; Fu()&#123; print("（5）父类构造器"); &#125; public static void print(String str)&#123; System.out.println(str + "-&gt;" + i); &#125; public static int getNum(String str)&#123; print(str); return ++i; &#125;&#125;class Zi extends Fu&#123; private static int k = getNum("（6）k"); private int h = getNum("（7）h"); static&#123; print("（8）子类静态代码块"); &#125; &#123; print("（9）子类非静态代码块，又称为构造代码块"); &#125; Zi()&#123; print("（10）子类构造器"); &#125; public static void print(String str)&#123; System.out.println(str + "-&gt;" + k); &#125; public static int getNum(String str)&#123; print(str); return ++k; &#125;&#125; 输出结果： 12345678910（1）i（3）父类静态代码块（6）k（8）子类静态代码块（2）j（4）父类非静态代码块，又称为构造代码块（10）子类构造器（7）h（9）子类非静态代码块，又称为构造代码块（10）子类构造器]]></content>
      <categories>
        <category>JavaSE</category>
      </categories>
      <tags>
        <tag>JavaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop分布式文件系统]]></title>
    <url>%2F2019%2F09%2F20%2FHadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1.Hadoop分布式文件系统1.1 HDFS的设计 HDFS以流式数据访问模式来存储超大文件。 流式数据访问：一次写入、多次读取 不适合低时间延迟的数据访问。HDFS是为高数据吞吐量应用优化的，可能会牺牲时间延迟。 不适合大量的小文件。由于namenode 将文件系统的元数据存储到内存中，因此namenode的内存容量决定了此文件系统所能存储的文件总数。 不适合多用户写入，任意修改文件的场景。HDFS的文件写入只支持单个写入，而且写操作都是”只添加”的方式写在文件末尾。 1.2 HDFS的概念数据块HDFS中块大小默认128MB，HDFS中小于一个块大小的文件不会占据整个块的空间。 HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。一般寻址时间占传输时间的1%。 NameNode和DataNode1）HDFS集群中有两类节点以管理节点-工作节点模式运行，即一个NameNode和多个DataNode。 ①NameNode管理文件系统的命名空间，以两个文件永久形式保存在本地磁盘上：Edits日志文件和FsImage镜像文件；②NameNode记录每个文件中各个块所在的数据节点信息，但它不会永久保存块的位置信息，因为这些信息会在系统启动时根据数据节点信息重建。 DataNode会根据需要存储并检索数据块，并且定期向NameNode发送他们所存储的块列表。 2）NameNode容错机制（两种） 备份那些组成文件系统元数据持久状态的文件。Hadoop可以配置NameNode在多个文件系统上保存元数据的持久状态。一般的配置是，将持久状态写入本地磁盘的同时，写入一个NFS。 运行SecondaryNameNode，但它不能被用作NameNode。SecondaryNameNode作用就是定期合并编辑日志与命名空间镜像。以防止编辑日志过大。 块缓存通常DataNode从磁盘中读取块，但对于访问频繁的文件，其对应的块可能被显式的缓存在DataNode的内存中，以堆外缓存的形式存在。 联邦HDFSNameNode在内存中保存文件系统中的每个文件和每个数据块的引用关系，这就意味着，内存将成为系统横向扩展的瓶颈。在2.x发行版本中允许系统通过添加NameNode来实现扩展，其中每个NameNode管理文件系统命名空间的一部分。 在联邦环境下，每个NameNode维护一个命名空间卷（namespace volume），由命名空间的元数据和一个数据块池（block pool）组成，数据块池包含该命名空间下文件的所有数据块。 HDFS的高可用性通过联邦HDFS和SecondaryNameNode能防止数据丢失，但是依旧无法实现文件系统的高可用性。NameNode依然存在单点失效（SPOF）问题，一旦NameNode失效，Hadoop系统无法提供服务直到有新的NameNode上线。 Hadoop2增加了HDFS高可用的支持，配置了Active-Standby NameNode，当活动NameNode失效，备用NameNode就会接管它的任务并开始服务，但需要在架构上做些修改： NameNode之间需要共享编辑日志，实现与活动NameNode的状态同步。 DataNode需要同时向两个NameNode发送数据处理报告。 辅助NameNode的角色被备用NameNode所包含，备用NameNode为活动的NameNode命名空间设置周期性检查点。 故障切换故障转移控制器（failover controller）管理将活动NameNode转移为备用NameNode的转移过程。有多种故障转移器。默认使用ZooKeeper来确保有且只有一个活动的NameNode，起哄做就是监视宿主NameNode是否失效（心跳机制）并在失效时进行故障切换。 1.3 Hadoop文件系统Hadoop有一个抽象的文件系统，HDFS只是其中的一个实现。org.apache.hadoop.hdfs.DistributedFileSystem 从Hadoop URL读取数据最简单的方法就是使用java.net.URL对象打开数据流，从中读取数据 1234567InputStream in = null;try&#123; in = new URL("hdfs://host/path").openStream(); IOUtils.copyBytes(in,System.out,4096,false);//流的对拷，4096-用于复制的缓冲区大小，false-复制结束后是否关闭数据流&#125;finally&#123; IOUtils.closeStream(in); //关闭数据流&#125; 通过FileSystem API读取数据Hadoop文件系统中通过Hadoop Path对象来代表文件，可以将路径视为一个Hadoop URI。 123public static FileSystem get(Configuration conf)public static FileSystem get(URI uri,Configuration conf)public static FileSystem get(URI uri,Configuration conf,String user) 有些情况，你可能需要获取本地文件系统的运行实例： 1public static LocalFileSystem getLocal(Configuration conf) 有了FileSystem实例，可以调用open函数来获取文件的输入流. 12public FSDataInputStream open(Path f) //默认使用缓冲区大小为4KBpublic abstract FSDataInputStream open(Path f,int bufferSize) 示例代码 1234567891011121314public class FileSystemCat&#123; public static void main(String[] args) throw Exception&#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri),conf); InputStream in = null; try&#123; in = fs.open(new Path(uri)); IOUtils.copyBytes(in,System.out,4096,false); &#125;finally&#123; IOUtils.closeStream(in); //关闭数据流 &#125; &#125;&#125; FSDataInputStreamFSDataInputStream是继承了java.io.DataInputStream，并支持随机访问，可以从流的任意位置读取数据 12345package org.apache.ahdoop.fs;public class FSDataInputStream extends DataInputStream implements Seekable,PositionReadable&#123; //...&#125; Seekable接口支持在文件中找到指定位置，并提供一个查询当前位置相对于文件起始位置偏移量（getPos()）的查询方法。 1.4 数据流文件读取 这个设计的一个重点是，客户端可以直接连接到DataNode检索数据，且NameNode告诉客户端每个块所在的最佳DataNode。 NameNode只需要响应块位置的请求（这些信息存储在内存中，非常高效），无需响应数据请求，不会因为客户端数量的增长使得NameNode成为瓶颈。 网络拓扑与Hadoop在海量数据处理中，其主要限制因素是节点之间数据的传输速率-网络带宽。如何衡量节点之间的带宽？ 节点距离：两个节点到达最近的共同祖先的距离总和。 distance(/d1/r1/n1,/d1/r1/n1) = 0 （同一节点上的进程） distance(/d1/r1/n1,/d1/r1/n2) = 2 （同一机架上的不同节点） distance(/d1/r1/n1,/d1/r2/n1) = 4 （同一数据中心不同机架上的节点） distance(/d1/r1/n1,/d2/r3/n4) = 6 （不同数据中心的节点） 机架感知（副本存储节点选择）1For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. 第一个副本在client所处的节点上，如果客户端在集群外，随机选一个； 第二个副本和第一个副本位于相同机架，随机节点； 第三个副本位于不同机架，随机节点。 文件写入]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell-重定向]]></title>
    <url>%2F2019%2F08%2F02%2FShell-%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[Shell输入输出重定向解析： 1nohup ./hiveserver2 &gt; /opt/module/hive/logs/server.log 2&gt;&amp;1 &amp; nohub和&amp;区别信号，是Unix系统中进程间通信的一种异步通信机制。 （1）SIGINT信号 （2）SIGHUB信号 Ctrl + C 产生SIGINT信号，关闭shell产生SIGHUB信号 &amp;：命令后面加一个“&amp;”，表示后台运行 &amp;只对SIGINT信号免疫，对SIGHUB信号不免疫 也就是说，&amp;在你使用Ctrl + C 关闭时，进程依然存活；但如果你把shell窗口关闭，进程就会被杀死。 nohub：忽略SIGHUB信号 单独使用nohub时，关闭窗口不会影响进程，但是Ctrl +C 操作会使得进程停止（对SIGHUB免疫，但是对SIGINT不免疫）。 总结要想使进程不受shell窗口关闭 和 Ctrl + C操作的影响，可以将nohup和&amp;指令一起使用。 1nohub ./xxx脚本 ... &amp; 重定向列表 命令 说明 command &gt; file 将输出重定向到 file command &lt; file 将输入重定向到 file command &gt;&gt; file 将输出以追加的方式重定向到 file n &gt; file 将文件描述符为 n 的文件重定向到 file n &gt;&gt; file 将文件描述符为 n 的文件以追加的方式重定向到 file n &gt;&amp; m 将输出文件 m 和 n 合并 n &lt;&amp; m 将输入文件 m 和 n 合并 重定向深入理解一般情况下，每个Unix和Linux命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息 默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。 如果希望 stderr 重定向到 file，可以这样写： 1$ command 2 &gt; file 2 表示标准错误文件(stderr)。 如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写： 12345$ command &gt; file 2&gt;&amp;1或者$ command &gt;&gt; file 2&gt;&amp;1 Linux xargs命令xargs 是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。 xargs 可以将管道或标准输入（stdin）数据转换成命令行参数，也能够从文件的输出中读取数据。 1somecommand |xargs -选项 command xargs 结合 find 使用 用 rm 删除太多的文件时候，可能得到一个错误信息：/bin/rm Argument list too long. 用 xargs 去避免这个问题： 1find . -type f -name "*.log" -print0 | xargs -0 rm -f]]></content>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.8源码-ArrayList]]></title>
    <url>%2F2019%2F08%2F01%2FJDK1.8%E6%BA%90%E7%A0%81-ArrayList%2F</url>
    <content type="text"><![CDATA[ArrayList 源码解析（1）JDK1.8：new ArrayList()：发现内部初始化为了一个长度为0的空数组 DEFAULTCAPACITY_EMPTY_ELEMENTDATA ​ JDK1.7版本：也是初始化为长度为0的空数组 EMPTY_ELEMENTDATA; ​ JDK1.6版本：初始化为长度为10的数组 为什么要初始化为空数组呢？ 因为开发中，很多时候创建了ArrayList的对象，但是没有装元素，这个时候的话，如果初始化为10的数组，就浪费空间了。 （2）什么时候扩容？– add(Object e） JDK1.8：第一次添加元素，扩容为长度为10的数组；如果不够了，再扩容为1.5倍 源码1234567891011121314151617181920212223242526272829303132333435363738394041424344private static final int DEFAULT_CAPACITY = 10;private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;//初始化为空数组&#125;public boolean add(E e) &#123; //查看当前数组是否够多存一个元素 ensureCapacityInternal(size + 1); // Increments modCount!! //存入新元素到[size]位置，然后size自增1 elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; //如果当前数组还是空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; //那么minCapacity取DEFAULT_CAPACITY与minCapacity的最大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; //查看是否需要扩容 ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++;//修改次数加1 // 如果需要的最小容量 比 当前数组的长度 大，即当前数组不够存，就扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length;//当前数组容量 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);//新数组容量是旧数组容量的1.5倍 //看旧数组的1.5倍是否够 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //看旧数组的1.5倍是否超过最大数组限制 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); //复制一个新数组 elementData = Arrays.copyOf(elementData, newCapacity); &#125; 1234567891011121314151617181920public E remove(int index) &#123; rangeCheck(index);//检验index是否合法 modCount++;//修改次数加1 //取出[index]位置的元素，[index]位置的元素就是要被删除的元素，用于最后返回被删除的元素 E oldValue = elementData(index); //需要移动的元素个数 int numMoved = size - index - 1; //如果需要移动元素，就用System.arraycopy移动元素 if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); //将elementData[size-1]位置置空，让GC回收空间，元素个数减少 elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 12345678910111213 public E set(int index, E element) &#123; rangeCheck(index);//检验index是否合法 //取出[index]位置的元素，[index]位置的元素就是要被替换的元素，用于最后返回被替换的元素 E oldValue = elementData(index); //用element替换[index]位置的元素 elementData[index] = element; return oldValue;&#125;public E get(int index) &#123; rangeCheck(index);//检验index是否合法 return elementData(index);//返回[index]位置的元素&#125; 12345678910111213141516171819202122232425262728public int indexOf(Object o) &#123; //分为o是否为空两种情况 if (o == null) &#123; //从前往后找 for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125;public int lastIndexOf(Object o) &#123; //分为o是否为空两种情况 if (o == null) &#123; //从后往前找 for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1;&#125;]]></content>
      <categories>
        <category>Java内功心法</category>
      </categories>
      <tags>
        <tag>JDK源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
